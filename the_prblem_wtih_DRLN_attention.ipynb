{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Laplacian Pyramid\"\n",
    "\n",
    "This notebook is ment to demonstrate my objections concerning the implementaiton of the laplacian pyramid in DRLN.\n",
    "Please remember their proposed desing of the attention mechanism in their paper. They aim to produce attention differenlty at each pyramid by using different downsampling layers.\n",
    "\n",
    "![Attention](figs/LapAtt.png)\n",
    "In the following cell is the original code of their implementation of the attention layer (CA). As you can see they used three different streames (c1,c2,c3) which are ment to be their \"pyramids\". However, the only parameters they change is the dilation and padding parameter of a 2D convolution as you can see in the BaiscBlock class they use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CALayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(CALayer, self).__init__()\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.c1 = ops.BasicBlock(channel , channel // reduction, 3, 1, 3, 3)\n",
    "        self.c2 = ops.BasicBlock(channel , channel // reduction, 3, 1, 5, 5)\n",
    "        self.c3 = ops.BasicBlock(channel , channel // reduction, 3, 1, 7, 7)\n",
    "        self.c4 = ops.BasicBlockSig((channel // reduction)*3, channel , 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape, \"input\")\n",
    "        y = self.avg_pool(x)\n",
    "        print(y.shape, \"after pooling\")\n",
    "        c1 = self.c1(y)\n",
    "        c2 = self.c2(y)\n",
    "        c3 = self.c3(y)\n",
    "        print(c1.shape, c2.shape, c3.shape, \"shapes\")\n",
    "        c_out = torch.cat([c1, c2, c3], dim=1)\n",
    "        y = self.c4(c_out)\n",
    "        return x * y\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels, out_channels,\n",
    "                 ksize=3, stride=1, pad=1, dilation=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, ksize, stride, pad, dilation),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        init_weights(self.modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But with a input of size (batch_size, channels,\n",
    "1, 1), padding it with the exact amount of zeros needed to fit the\n",
    "increased dilation will render all weights of the convolution layers\n",
    "except the center ones useless. Thus, it seems to me that they do the\n",
    "same computational process in each pyramid which is also the same as the\n",
    "simple CA used by RCAN but with introducing unnecessary weights (i.e.\n",
    "weights that always only get the input zero from the padding) by\n",
    "changing padding and dilation. I tried it myself, and came to the\n",
    "conclusion that one could produce the same result with just using 1x1\n",
    "kernels without padding and dilation, thus getting rid of quite a few\n",
    "parameters.\n",
    "I tried to produce a simple visualization of my remark (see picture below) and wrote the following code to support my claim.\n",
    "![](figs/DRLN_cnn_dilation_and_padding.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the different layers in the Attention Model\n",
    "channels_in = 64\n",
    "reduction = 16\n",
    "channels_out = channels_in // reduction\n",
    "dilation3 = nn.Conv2d(channels_in, channels_out, 3,1,3,3, bias=False)\n",
    "dilation5 = nn.Conv2d(channels_in, channels_out, 3,1,5,5, bias=False)\n",
    "dilation7 = nn.Conv2d(channels_in, channels_out, 3,1,7,7, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), bias=False) weights: Parameter containing:\n",
      "tensor([[[[ 0.0194,  0.0082, -0.0070],\n",
      "          [-0.0206, -0.0349, -0.0054],\n",
      "          [-0.0237,  0.0378, -0.0342]],\n",
      "\n",
      "         [[ 0.0061, -0.0162, -0.0329],\n",
      "          [-0.0229, -0.0115,  0.0208],\n",
      "          [ 0.0105,  0.0258, -0.0354]],\n",
      "\n",
      "         [[ 0.0118, -0.0330,  0.0227],\n",
      "          [ 0.0015, -0.0335,  0.0152],\n",
      "          [ 0.0045, -0.0139, -0.0201]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0361,  0.0002, -0.0052],\n",
      "          [ 0.0041, -0.0185, -0.0294],\n",
      "          [ 0.0240,  0.0122,  0.0228]],\n",
      "\n",
      "         [[ 0.0299, -0.0288,  0.0032],\n",
      "          [ 0.0295,  0.0335, -0.0177],\n",
      "          [-0.0142,  0.0159,  0.0390]],\n",
      "\n",
      "         [[ 0.0102, -0.0335, -0.0273],\n",
      "          [-0.0058,  0.0353,  0.0183],\n",
      "          [-0.0052,  0.0245, -0.0352]]],\n",
      "\n",
      "\n",
      "        [[[-0.0015, -0.0266, -0.0134],\n",
      "          [ 0.0318,  0.0102, -0.0257],\n",
      "          [ 0.0395,  0.0312,  0.0364]],\n",
      "\n",
      "         [[-0.0358, -0.0256, -0.0186],\n",
      "          [ 0.0016,  0.0321,  0.0236],\n",
      "          [ 0.0287, -0.0045, -0.0219]],\n",
      "\n",
      "         [[ 0.0223,  0.0245,  0.0031],\n",
      "          [-0.0329,  0.0064, -0.0247],\n",
      "          [-0.0064, -0.0184,  0.0009]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0326, -0.0289,  0.0299],\n",
      "          [ 0.0214,  0.0380, -0.0347],\n",
      "          [ 0.0228,  0.0146,  0.0250]],\n",
      "\n",
      "         [[-0.0384,  0.0286,  0.0162],\n",
      "          [-0.0300, -0.0321,  0.0075],\n",
      "          [ 0.0158, -0.0172,  0.0115]],\n",
      "\n",
      "         [[-0.0206, -0.0385, -0.0315],\n",
      "          [-0.0140, -0.0373, -0.0086],\n",
      "          [ 0.0299,  0.0067,  0.0154]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0231,  0.0290, -0.0066],\n",
      "          [ 0.0089, -0.0410,  0.0213],\n",
      "          [-0.0295, -0.0129,  0.0375]],\n",
      "\n",
      "         [[-0.0128, -0.0409,  0.0217],\n",
      "          [ 0.0413,  0.0401,  0.0224],\n",
      "          [ 0.0109, -0.0163,  0.0317]],\n",
      "\n",
      "         [[-0.0158, -0.0026,  0.0173],\n",
      "          [ 0.0088, -0.0402,  0.0068],\n",
      "          [ 0.0292,  0.0027, -0.0232]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0172, -0.0022, -0.0166],\n",
      "          [ 0.0194, -0.0291, -0.0398],\n",
      "          [ 0.0083, -0.0320,  0.0376]],\n",
      "\n",
      "         [[-0.0216,  0.0252, -0.0327],\n",
      "          [-0.0054, -0.0282, -0.0411],\n",
      "          [ 0.0367, -0.0021,  0.0416]],\n",
      "\n",
      "         [[-0.0334, -0.0338,  0.0191],\n",
      "          [-0.0096,  0.0323,  0.0110],\n",
      "          [ 0.0024,  0.0171, -0.0057]]],\n",
      "\n",
      "\n",
      "        [[[-0.0372,  0.0114, -0.0107],\n",
      "          [ 0.0237, -0.0017,  0.0354],\n",
      "          [-0.0100,  0.0373,  0.0044]],\n",
      "\n",
      "         [[ 0.0177,  0.0166, -0.0035],\n",
      "          [-0.0096,  0.0104,  0.0083],\n",
      "          [ 0.0117, -0.0389,  0.0248]],\n",
      "\n",
      "         [[ 0.0076, -0.0385,  0.0327],\n",
      "          [-0.0394, -0.0253, -0.0201],\n",
      "          [ 0.0170,  0.0086,  0.0315]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0372, -0.0305, -0.0158],\n",
      "          [ 0.0226,  0.0291,  0.0271],\n",
      "          [-0.0359,  0.0063, -0.0042]],\n",
      "\n",
      "         [[ 0.0093,  0.0376,  0.0363],\n",
      "          [ 0.0259,  0.0329, -0.0108],\n",
      "          [-0.0310,  0.0394,  0.0246]],\n",
      "\n",
      "         [[ 0.0260,  0.0370,  0.0332],\n",
      "          [-0.0032, -0.0139, -0.0118],\n",
      "          [ 0.0280, -0.0398, -0.0415]]]], requires_grad=True) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False) weights: Parameter containing:\n",
      "tensor([[[[ 0.0334, -0.0383,  0.0219],\n",
      "          [-0.0384, -0.0272, -0.0403],\n",
      "          [-0.0080,  0.0108,  0.0226]],\n",
      "\n",
      "         [[-0.0344, -0.0355, -0.0150],\n",
      "          [ 0.0412, -0.0196,  0.0364],\n",
      "          [ 0.0344,  0.0321, -0.0218]],\n",
      "\n",
      "         [[-0.0284,  0.0263, -0.0415],\n",
      "          [-0.0319,  0.0350,  0.0264],\n",
      "          [-0.0150,  0.0282,  0.0143]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0136,  0.0315, -0.0074],\n",
      "          [ 0.0008,  0.0015, -0.0139],\n",
      "          [-0.0179,  0.0030, -0.0225]],\n",
      "\n",
      "         [[ 0.0219, -0.0415, -0.0317],\n",
      "          [ 0.0292, -0.0289, -0.0315],\n",
      "          [-0.0384,  0.0329,  0.0077]],\n",
      "\n",
      "         [[ 0.0153,  0.0211,  0.0051],\n",
      "          [ 0.0227, -0.0020, -0.0386],\n",
      "          [-0.0252, -0.0264,  0.0167]]],\n",
      "\n",
      "\n",
      "        [[[-0.0191, -0.0251, -0.0404],\n",
      "          [ 0.0070,  0.0151,  0.0090],\n",
      "          [-0.0230, -0.0381,  0.0009]],\n",
      "\n",
      "         [[-0.0289,  0.0127, -0.0261],\n",
      "          [-0.0036,  0.0050,  0.0354],\n",
      "          [ 0.0117, -0.0077, -0.0157]],\n",
      "\n",
      "         [[-0.0017,  0.0394,  0.0274],\n",
      "          [-0.0063, -0.0190,  0.0144],\n",
      "          [ 0.0302,  0.0094, -0.0381]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0359,  0.0188,  0.0310],\n",
      "          [-0.0125,  0.0289, -0.0242],\n",
      "          [-0.0158, -0.0401, -0.0312]],\n",
      "\n",
      "         [[-0.0220, -0.0142, -0.0194],\n",
      "          [-0.0183, -0.0190,  0.0076],\n",
      "          [-0.0069, -0.0092,  0.0329]],\n",
      "\n",
      "         [[-0.0114, -0.0243, -0.0008],\n",
      "          [ 0.0135,  0.0139,  0.0318],\n",
      "          [ 0.0188, -0.0077,  0.0172]]],\n",
      "\n",
      "\n",
      "        [[[-0.0081,  0.0291, -0.0274],\n",
      "          [ 0.0137, -0.0403, -0.0260],\n",
      "          [ 0.0395,  0.0250,  0.0371]],\n",
      "\n",
      "         [[-0.0086, -0.0182,  0.0324],\n",
      "          [ 0.0213,  0.0368,  0.0074],\n",
      "          [-0.0230,  0.0268, -0.0072]],\n",
      "\n",
      "         [[-0.0368,  0.0208, -0.0011],\n",
      "          [-0.0191, -0.0061, -0.0258],\n",
      "          [-0.0411,  0.0087,  0.0073]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0002, -0.0200, -0.0244],\n",
      "          [ 0.0044,  0.0159, -0.0235],\n",
      "          [-0.0206, -0.0388,  0.0066]],\n",
      "\n",
      "         [[-0.0357, -0.0157,  0.0036],\n",
      "          [-0.0369, -0.0189,  0.0380],\n",
      "          [-0.0005,  0.0028,  0.0013]],\n",
      "\n",
      "         [[ 0.0081,  0.0416, -0.0089],\n",
      "          [ 0.0006, -0.0407,  0.0384],\n",
      "          [-0.0191,  0.0134, -0.0065]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0311,  0.0245, -0.0348],\n",
      "          [-0.0337,  0.0305,  0.0353],\n",
      "          [ 0.0064, -0.0313,  0.0352]],\n",
      "\n",
      "         [[-0.0051,  0.0343,  0.0404],\n",
      "          [ 0.0020, -0.0028,  0.0152],\n",
      "          [ 0.0283, -0.0270,  0.0219]],\n",
      "\n",
      "         [[-0.0275,  0.0119,  0.0024],\n",
      "          [-0.0415, -0.0308, -0.0076],\n",
      "          [-0.0106, -0.0182, -0.0019]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0041,  0.0003, -0.0360],\n",
      "          [ 0.0155, -0.0388,  0.0234],\n",
      "          [-0.0274,  0.0402, -0.0131]],\n",
      "\n",
      "         [[-0.0260,  0.0309, -0.0342],\n",
      "          [-0.0120,  0.0007,  0.0076],\n",
      "          [ 0.0333,  0.0302,  0.0383]],\n",
      "\n",
      "         [[ 0.0159, -0.0253, -0.0063],\n",
      "          [ 0.0377,  0.0090,  0.0119],\n",
      "          [-0.0037, -0.0151, -0.0062]]]], requires_grad=True) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(7, 7), dilation=(7, 7), bias=False) weights: Parameter containing:\n",
      "tensor([[[[ 5.4319e-03,  1.9589e-02,  1.0669e-02],\n",
      "          [ 2.7258e-02,  9.7908e-03,  8.1698e-03],\n",
      "          [ 3.1973e-02, -4.1451e-02,  2.9187e-02]],\n",
      "\n",
      "         [[-1.2865e-02,  4.1659e-02, -5.3127e-03],\n",
      "          [ 3.2640e-02,  2.3222e-02, -1.8916e-02],\n",
      "          [-3.4803e-02, -3.6221e-02, -3.7443e-02]],\n",
      "\n",
      "         [[ 4.4190e-03, -1.7453e-02,  2.6793e-02],\n",
      "          [ 2.7553e-02,  8.4997e-03,  3.1316e-02],\n",
      "          [-2.1360e-02,  1.4406e-02, -3.4719e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.1685e-03,  2.4475e-02,  2.9904e-02],\n",
      "          [ 7.4754e-04,  2.8775e-02, -2.2354e-02],\n",
      "          [-3.2482e-02,  3.0694e-03,  1.4204e-02]],\n",
      "\n",
      "         [[-4.1077e-02,  3.6670e-02,  1.9643e-02],\n",
      "          [-2.5680e-02, -2.5353e-02, -1.6482e-02],\n",
      "          [ 1.7465e-02,  3.7068e-02, -3.4841e-02]],\n",
      "\n",
      "         [[ 3.5350e-02,  3.1697e-02,  3.4612e-02],\n",
      "          [-3.8580e-02, -1.7689e-02,  2.7789e-02],\n",
      "          [-1.8059e-02,  3.7767e-02, -7.5810e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.0476e-02,  2.3054e-02,  7.9702e-04],\n",
      "          [ 2.8925e-02,  2.3421e-02,  3.1822e-02],\n",
      "          [-3.4717e-02, -3.3972e-02,  3.6085e-02]],\n",
      "\n",
      "         [[-3.9863e-02,  2.6767e-02, -1.1166e-02],\n",
      "          [-3.4872e-03,  2.0866e-02, -3.9269e-02],\n",
      "          [ 3.0790e-02,  2.1015e-02, -1.4565e-02]],\n",
      "\n",
      "         [[ 1.2290e-03,  2.6455e-02, -4.8106e-03],\n",
      "          [ 3.8497e-03, -2.1501e-02, -2.0052e-02],\n",
      "          [ 1.3868e-02,  2.0915e-02, -3.6084e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.0071e-02, -2.1543e-02, -1.2633e-02],\n",
      "          [-5.5237e-03,  1.9747e-02, -2.4482e-02],\n",
      "          [-1.3896e-02,  2.1760e-02, -2.9663e-02]],\n",
      "\n",
      "         [[ 3.9622e-03,  2.6052e-02,  3.6307e-02],\n",
      "          [-1.9408e-02,  3.0246e-04, -3.8580e-02],\n",
      "          [ 1.6216e-02,  1.6950e-02, -3.9396e-02]],\n",
      "\n",
      "         [[ 3.1332e-02,  1.8830e-03,  1.5940e-03],\n",
      "          [-3.6832e-02, -1.7590e-02,  7.9839e-03],\n",
      "          [ 1.8190e-02,  1.5014e-04,  3.8492e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.5696e-02, -1.1712e-02, -1.5203e-03],\n",
      "          [ 8.8142e-03, -9.8094e-03, -3.4799e-03],\n",
      "          [ 1.9611e-02, -1.9820e-02, -4.0382e-04]],\n",
      "\n",
      "         [[-3.0672e-02,  2.1124e-02,  1.5050e-02],\n",
      "          [ 3.7521e-02, -2.8612e-02,  2.4790e-02],\n",
      "          [-1.1998e-03,  1.8958e-02, -3.0515e-02]],\n",
      "\n",
      "         [[ 2.3381e-02,  2.7926e-02,  2.5321e-02],\n",
      "          [-2.9552e-02,  2.5705e-02,  2.2552e-02],\n",
      "          [ 1.2091e-02, -1.4177e-02,  2.4975e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6581e-02,  2.0616e-02,  2.7429e-02],\n",
      "          [-2.5225e-02,  2.2219e-02,  3.8624e-02],\n",
      "          [ 2.3093e-02, -2.3568e-02,  2.8590e-02]],\n",
      "\n",
      "         [[-1.4331e-02,  3.9225e-02,  9.7616e-03],\n",
      "          [ 3.7941e-02, -4.7163e-03, -2.2664e-02],\n",
      "          [-4.0880e-02,  2.4070e-03,  2.4609e-02]],\n",
      "\n",
      "         [[ 3.8696e-02,  4.4191e-04,  2.3835e-02],\n",
      "          [ 2.8158e-02,  8.7951e-03, -7.0121e-05],\n",
      "          [-3.3035e-02,  1.6377e-02, -2.6380e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1850e-02, -1.2365e-02,  3.1924e-02],\n",
      "          [-1.7331e-03, -1.9209e-02,  7.3553e-03],\n",
      "          [ 3.5190e-02,  8.3675e-03, -2.9062e-02]],\n",
      "\n",
      "         [[-4.0373e-02, -1.2567e-02, -3.2593e-03],\n",
      "          [ 1.7160e-02, -2.2026e-02, -7.4532e-03],\n",
      "          [-1.3907e-02, -1.8396e-02,  4.0645e-02]],\n",
      "\n",
      "         [[ 3.8482e-02,  1.9855e-02, -1.8690e-02],\n",
      "          [ 2.8846e-02, -3.9506e-02,  3.9316e-02],\n",
      "          [-3.6152e-03,  3.3589e-03,  3.5176e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0473e-03,  2.7607e-02, -1.1397e-02],\n",
      "          [-4.0753e-02, -4.0973e-03, -1.9913e-02],\n",
      "          [-1.0775e-02,  3.5568e-02,  2.9125e-02]],\n",
      "\n",
      "         [[ 3.9414e-02,  2.9263e-02,  2.2364e-02],\n",
      "          [ 1.9538e-02,  1.5735e-03,  6.4971e-03],\n",
      "          [ 8.8886e-03,  1.1810e-02,  4.0626e-02]],\n",
      "\n",
      "         [[-2.3486e-02,  4.4376e-03, -1.4206e-02],\n",
      "          [ 2.0452e-02, -3.8028e-02, -3.9806e-03],\n",
      "          [ 1.7044e-02, -2.9275e-02, -6.2674e-03]]]], requires_grad=True) \n"
     ]
    }
   ],
   "source": [
    "# setting the weights to 1\n",
    "conv_all_one = []\n",
    "for conv in [dilation3, dilation5, dilation7]:\n",
    "    #conv.weight = nn.Parameter(torch.ones(conv.weight.shape))\n",
    "    print(f\"{conv} weights: {conv.weight} \")\n",
    "    conv_all_one.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy input has chape torch.Size([1, 64, 1, 1])\n",
      "tensor([[[[ 0.]],\n",
      "\n",
      "         [[ 1.]],\n",
      "\n",
      "         [[ 2.]],\n",
      "\n",
      "         [[ 3.]],\n",
      "\n",
      "         [[ 4.]],\n",
      "\n",
      "         [[ 5.]],\n",
      "\n",
      "         [[ 6.]],\n",
      "\n",
      "         [[ 7.]],\n",
      "\n",
      "         [[ 8.]],\n",
      "\n",
      "         [[ 9.]],\n",
      "\n",
      "         [[10.]],\n",
      "\n",
      "         [[11.]],\n",
      "\n",
      "         [[12.]],\n",
      "\n",
      "         [[13.]],\n",
      "\n",
      "         [[14.]],\n",
      "\n",
      "         [[15.]],\n",
      "\n",
      "         [[16.]],\n",
      "\n",
      "         [[17.]],\n",
      "\n",
      "         [[18.]],\n",
      "\n",
      "         [[19.]],\n",
      "\n",
      "         [[20.]],\n",
      "\n",
      "         [[21.]],\n",
      "\n",
      "         [[22.]],\n",
      "\n",
      "         [[23.]],\n",
      "\n",
      "         [[24.]],\n",
      "\n",
      "         [[25.]],\n",
      "\n",
      "         [[26.]],\n",
      "\n",
      "         [[27.]],\n",
      "\n",
      "         [[28.]],\n",
      "\n",
      "         [[29.]],\n",
      "\n",
      "         [[30.]],\n",
      "\n",
      "         [[31.]],\n",
      "\n",
      "         [[32.]],\n",
      "\n",
      "         [[33.]],\n",
      "\n",
      "         [[34.]],\n",
      "\n",
      "         [[35.]],\n",
      "\n",
      "         [[36.]],\n",
      "\n",
      "         [[37.]],\n",
      "\n",
      "         [[38.]],\n",
      "\n",
      "         [[39.]],\n",
      "\n",
      "         [[40.]],\n",
      "\n",
      "         [[41.]],\n",
      "\n",
      "         [[42.]],\n",
      "\n",
      "         [[43.]],\n",
      "\n",
      "         [[44.]],\n",
      "\n",
      "         [[45.]],\n",
      "\n",
      "         [[46.]],\n",
      "\n",
      "         [[47.]],\n",
      "\n",
      "         [[48.]],\n",
      "\n",
      "         [[49.]],\n",
      "\n",
      "         [[50.]],\n",
      "\n",
      "         [[51.]],\n",
      "\n",
      "         [[52.]],\n",
      "\n",
      "         [[53.]],\n",
      "\n",
      "         [[54.]],\n",
      "\n",
      "         [[55.]],\n",
      "\n",
      "         [[56.]],\n",
      "\n",
      "         [[57.]],\n",
      "\n",
      "         [[58.]],\n",
      "\n",
      "         [[59.]],\n",
      "\n",
      "         [[60.]],\n",
      "\n",
      "         [[61.]],\n",
      "\n",
      "         [[62.]],\n",
      "\n",
      "         [[63.]]]])\n"
     ]
    }
   ],
   "source": [
    "# defining a dummy input of shape (1 (batch_size), channels in, 1, 1)\n",
    "arr = np.arange(channels_in).reshape(1,channels_in, 1, 1)\n",
    "dummy = torch.tensor(arr, dtype=torch.float)\n",
    "print(f\"dummy input has chape {dummy.shape}\")\n",
    "print(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we do the forward pass with our 3 cnn layers\n",
    "def compute_result(dummy, dilations):\n",
    "    results = []\n",
    "    for dil in dilations:\n",
    "        results.append(dil(dummy))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[  4.4085]],\n",
      "\n",
      "         [[  0.6269]],\n",
      "\n",
      "         [[-12.6424]],\n",
      "\n",
      "         [[  6.8819]]]], grad_fn=<SlowConvDilated2DBackward>), tensor([[[[ 1.6467]],\n",
      "\n",
      "         [[-0.5371]],\n",
      "\n",
      "         [[-2.9924]],\n",
      "\n",
      "         [[-6.8759]]]], grad_fn=<SlowConvDilated2DBackward>), tensor([[[[  1.0048]],\n",
      "\n",
      "         [[  6.6578]],\n",
      "\n",
      "         [[  3.4085]],\n",
      "\n",
      "         [[-12.9482]]]], grad_fn=<SlowConvDilated2DBackward>)]\n"
     ]
    }
   ],
   "source": [
    "results_all_one = compute_result(dummy, conv_all_one)\n",
    "print(results_all_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now sett all weights to 0.1 the output will just be a tensor of shape (1,4,1,1) with all values being 0.1 * 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), bias=False) weights: Parameter containing:\n",
      "tensor([[[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]]], grad_fn=<CopySlices>) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False) weights: Parameter containing:\n",
      "tensor([[[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]]], grad_fn=<CopySlices>) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(7, 7), dilation=(7, 7), bias=False) weights: Parameter containing:\n",
      "tensor([[[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]]], grad_fn=<CopySlices>) \n"
     ]
    }
   ],
   "source": [
    "conv_all_point_one = []\n",
    "for conv in [dilation3, dilation5, dilation7]:\n",
    "    conv.weight = nn.Parameter(torch.ones(conv.weight.shape))\n",
    "    conv.weight[:] = 0.1\n",
    "    print(f\"{conv} weights: {conv.weight} \")\n",
    "    conv_all_point_one.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]]]], grad_fn=<SlowConvDilated2DBackward>), tensor([[[[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]]]], grad_fn=<SlowConvDilated2DBackward>), tensor([[[[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]]]], grad_fn=<SlowConvDilated2DBackward>)]\n"
     ]
    }
   ],
   "source": [
    "results_all_point_one = compute_result(dummy, conv_all_point_one)\n",
    "print(results_all_point_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), bias=False) weights: Parameter containing:\n",
      "tensor([[[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]]], grad_fn=<CopySlices>) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False) weights: Parameter containing:\n",
      "tensor([[[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]]], grad_fn=<CopySlices>) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(7, 7), dilation=(7, 7), bias=False) weights: Parameter containing:\n",
      "tensor([[[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]],\n",
      "\n",
      "\n",
      "        [[[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]],\n",
      "\n",
      "         [[0.1000, 0.1000, 0.1000],\n",
      "          [0.1000, 1.0000, 0.1000],\n",
      "          [0.1000, 0.1000, 0.1000]]]], grad_fn=<CopySlices>) \n"
     ]
    }
   ],
   "source": [
    "# now we set all weights to 0.1 except the one in the iddle which will be 1\n",
    "conv_one_one = []\n",
    "for conv in [dilation3, dilation5, dilation7]:\n",
    "    conv.weight[:] = 0.1\n",
    "    # the middle weight determines the output\n",
    "    # 1 -> 2016\n",
    "    conv.weight[:,:,1,1] = 1\n",
    "    # everything else will not matter\n",
    "    print(f\"{conv} weights: {conv.weight} \")\n",
    "    conv_one_one.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]]]], grad_fn=<SlowConvDilated2DBackward>), tensor([[[[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]]]], grad_fn=<SlowConvDilated2DBackward>), tensor([[[[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]]]], grad_fn=<SlowConvDilated2DBackward>)]\n"
     ]
    }
   ],
   "source": [
    "results_one_one = compute_result(dummy, conv_one_one)\n",
    "print(results_one_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact we can choos all the weights we want for the non-center weights of our kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3), bias=False) weights: Parameter containing:\n",
      "tensor([[[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 3.1416e+00]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]]], grad_fn=<CopySlices>) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(5, 5), dilation=(5, 5), bias=False) weights: Parameter containing:\n",
      "tensor([[[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 3.1416e+00]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]]], grad_fn=<CopySlices>) \n",
      "Conv2d(64, 4, kernel_size=(3, 3), stride=(1, 1), padding=(7, 7), dilation=(7, 7), bias=False) weights: Parameter containing:\n",
      "tensor([[[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 3.1416e+00]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[0.0000e+00, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]],\n",
      "\n",
      "\n",
      "        [[[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e+16]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]],\n",
      "\n",
      "         [[1.0000e-01, 1.0000e-01, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e+00, 1.0000e-01],\n",
      "          [1.0000e-01, 1.0000e-01, 1.0000e-01]]]], grad_fn=<CopySlices>) \n"
     ]
    }
   ],
   "source": [
    "# now we set all weights to 0.1 except the one in the iddle which will be 1\n",
    "conv_one_one_rest_random = []\n",
    "for conv in [dilation3, dilation5, dilation7]:\n",
    "    conv.weight[:] = 0.1\n",
    "    # the middle weight determines the output\n",
    "    # 1 -> 2016\n",
    "    conv.weight[:,:,1,1] = 1\n",
    "    # everything else will not matter\n",
    "    conv.weight[:,0,2,2] = 10000000000000000\n",
    "    conv.weight[2,1,0,0] = 0\n",
    "    conv.weight[1,1,2,2] = np.pi\n",
    "    print(f\"{conv} weights: {conv.weight} \")\n",
    "    conv_one_one_rest_random.append(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]]]], grad_fn=<SlowConvDilated2DBackward>), tensor([[[[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]]]], grad_fn=<SlowConvDilated2DBackward>), tensor([[[[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]],\n",
      "\n",
      "         [[2016.]]]], grad_fn=<SlowConvDilated2DBackward>)]\n"
     ]
    }
   ],
   "source": [
    "results_one_one_rest_random = compute_result(dummy, conv_one_one_rest_random)\n",
    "print(results_one_one_rest_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that each output has the same dimensions and the same values.\n",
    "Morover, only changing the dilation and the padding value will render all weights of our dilation useless except the one in the middle which will be multiplied with our 1x1 input.\n",
    "    We can demonstrate that by setting all weights to 0 except the one in the middle. Thus we could achive the same when using convolutions with 1x1 kernels instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False) weights: torch.Size([4, 64, 1, 1]) \n",
      "Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False) weights: torch.Size([4, 64, 1, 1]) \n",
      "Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False) weights: torch.Size([4, 64, 1, 1]) \n"
     ]
    }
   ],
   "source": [
    "channels_in = 64\n",
    "reduction = 16\n",
    "channels_out = channels_in // reduction\n",
    "simple_layers = []\n",
    "for i in range(3):\n",
    "    simple_layers.append(nn.Conv2d(channels_in, channels_out, 1, bias=False))\n",
    "    simple_layers[i].weight = nn.Parameter(torch.ones(simple_layers[i].weight.shape))\n",
    "    simple_layers[i].weight[:] = 0.1 \n",
    "    print(f\"{simple_layers[i]} weights: {simple_layers[i].weight.shape} \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]]]], grad_fn=<MkldnnConvolutionBackward>), tensor([[[[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]]]], grad_fn=<MkldnnConvolutionBackward>), tensor([[[[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]],\n",
      "\n",
      "         [[201.6000]]]], grad_fn=<MkldnnConvolutionBackward>)]\n"
     ]
    }
   ],
   "source": [
    "results = compute_result(dummy, simple_layers)\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
